{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "985f0dc3",
        "outputId": "8b86ce21-0a4c-459b-a169-6ed2d65de5d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install openai numpy matplotlib pydantic --upgrade --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0efb691e-15c8-4fe5-a27a-1704c7852d6d"
      },
      "source": [
        "# Comparing Chat Completions & Responses APIs\n",
        "\n",
        "This notebook explains the differences between two approaches for interacting with the OpenAI chat API:\n",
        "\n",
        "1. **Chat Completions API (`chat.completions.create()`)**: A more *raw* interface where you provide a list of messages. In this model, you must handle the conversation state manually if you want to carry context from one turn to the next.\n",
        "\n",
        "2. **Responses API (`responses.create()`)**: A newer, stateful API that not only lets you send messages but also automatically manages conversation state, making it easier to chain requests. This API is designed to be used with agents and will eventually replace the Assistants API.\n",
        "\n",
        "Below, we'll break down the differences in arguments and design, and then look at a minimal code example for each."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5dc84cf2-4ee4-4c67-bb8f-ae3a741d0e85"
      },
      "source": [
        "## Key Differences\n",
        "\n",
        "- **State Management**:\n",
        "  - **Chat Completions**: You provide a `messages` array; every call is stateless unless you manually add previous messages.\n",
        "  - **Responses API**: Automatically manages conversation state. You can chain requests easily using parameters like `store` and `previous_response_id`.\n",
        "\n",
        "- **Arguments & Structure**:\n",
        "  - **Chat Completions**: Mainly accepts a `messages` array with objects having keys like `role` and `content`. It returns results in a `choices` list, where you get the output from `message.content`.\n",
        "  - **Responses API**: Uses an `input` parameter (or an array of message objects) and returns a structured `output` which aggregates results. It also provides additional state management features.\n",
        "\n",
        "- **Future Direction**:\n",
        "  - The Responses API is designed for building agents and will replace the current Assistants API. It offers a smoother developer experience by holding the chat context between turns.\n",
        "\n",
        "In summary, while the Chat Completions endpoint gives you direct control but requires manual state management, the Responses API simplifies multi-turn interactions and is better suited for advanced applications."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bb259311-7198-4a5b-8133-52a782e7b7d2"
      },
      "source": [
        "## Minimal Code Example: Chat Completions API\n",
        "\n",
        "This example shows a typical call to `chat.completions.create()`. Note that if you want to carry context from one call to the next, you must manually append previous messages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "49e3a4af"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "MODEL = \"gpt-4.1-mini\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0451c1c1"
      },
      "outputs": [],
      "source": [
        "# Initialize the client\n",
        "# Note: In a real application, you would use an environment variable or secure method\n",
        "# to store your API key. This is just for demonstration.\n",
        "client = OpenAI(\n",
        "    # Replace with your actual API key or use: api_key=os.environ.get(\"OPENAI_API_KEY\")\n",
        "    api_key=\"YOUR_API_KEY_HERE\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e6c0b774-e4b7-447a-93ef-96c63a1c2e9e",
        "outputId": "74218f98-7ddf-4656-9aba-ca7c71ec1f67"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chat Completions response: Why did the scarecrow win an award? \n",
            "\n",
            "Because he was outstanding in his field!\n",
            "Follow-up response: Why don't skeletons fight each other? \n",
            "\n",
            "They don't have the guts!\n"
          ]
        }
      ],
      "source": [
        "# A single-turn conversation using chat completions\n",
        "response = client.chat.completions.create(\n",
        "    model=MODEL,\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": \"Tell me a joke.\"}\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(\"Chat Completions response:\", response.choices[0].message.content)\n",
        "\n",
        "# For multiple turns, you would manually append previous messages:\n",
        "conversation = [\n",
        "    {\"role\": \"user\", \"content\": \"Tell me a joke.\"},\n",
        "    {\"role\": \"assistant\", \"content\": response.choices[0].message.content}\n",
        "]\n",
        "\n",
        "# Follow-up request\n",
        "followup = client.chat.completions.create(\n",
        "    model=MODEL,\n",
        "    messages=conversation + [{\"role\": \"user\", \"content\": \"Tell me another joke.\"}]\n",
        ")\n",
        "\n",
        "print(\"Follow-up response:\", followup.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d19fad44-e502-4472-9baa-3f7b5bb1bb0f"
      },
      "source": [
        "## Minimal Code Example: Responses API\n",
        "\n",
        "This example uses the Responses API. Notice that it uses the `input` parameter instead of `messages` and can easily chain conversation state—this makes it well suited for agent applications."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "47b2e484-4e6b-4be5-80eb-2b80eb59c8a2",
        "outputId": "bb2319fe-6076-4f7b-eb64-b88a1ebe5b8c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Responses API response: Sure! Here's one for you:\n",
            "\n",
            "Why don’t scientists trust atoms?\n",
            "\n",
            "Because they make up everything!\n",
            "Chained response: Of course! Here’s another one:\n",
            "\n",
            "Why did the scarecrow win an award?\n",
            "\n",
            "Because he was outstanding in his field!\n"
          ]
        }
      ],
      "source": [
        "# A single-turn conversation using the Responses API\n",
        "response = client.responses.create(\n",
        "    model=MODEL,\n",
        "    input=[{\"role\": \"user\", \"content\": \"Tell me a joke.\"}]\n",
        ")\n",
        "\n",
        "print(\"Responses API response:\", response.output_text)\n",
        "\n",
        "# Chaining conversation state automatically (the state is held internally):\n",
        "second_response = client.responses.create(\n",
        "    model=MODEL,\n",
        "    previous_response_id=response.id,\n",
        "    input=[{\"role\": \"user\", \"content\": \"Tell me another joke.\"}]\n",
        ")\n",
        "\n",
        "print(\"Chained response:\", second_response.output_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18511dea-d73d-44a6-aa4c-7b57c3490c1b"
      },
      "source": [
        "## Summary\n",
        "\n",
        "- **Chat Completions API:**\n",
        "  - Requires manual handling of conversation state (append messages by yourself).\n",
        "  - Uses the `messages` parameter and returns output under `choices` → `message.content`.\n",
        "\n",
        "- **Responses API:**\n",
        "  - Manages conversation state automatically, easing the chaining of multi-turn interactions.\n",
        "  - Uses the `input` parameter and returns aggregated output as `output_text`.\n",
        "  - Will be the API of choice for building agents and is set to replace the current Assistants API.\n",
        "\n",
        "By choosing between these two, you can decide whether you prefer raw control (and manual state management) or a streamlined, stateful approach for more complex, agent-like interactions."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "MODEL = \"gpt-4o-mini\"\n",
        "\n",
        "client = OpenAI(api_key = \"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\")\n",
        "response = client.responses.create(\n",
        "    model = MODEL,\n",
        "    input = [{\"role\": \"user\", \"content\": \"Cho tôi biết thủ đô của Malaysia!\"}]\n",
        ")\n",
        "print(response.output_text)\n",
        "print(\"----------------------\")\n",
        "response_2 = client.responses.create(\n",
        "    model = MODEL,\n",
        "    previous_response_id = response.id,\n",
        "    input = [{\"role\": \"user\", \"content\": \"Thế còn Trung Quốc thì saoo?\"}]\n",
        ")\n",
        "print(response_2.output_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dHfxAR1Ygb4S",
        "outputId": "7faa380a-06c5-426c-d263-3d4c5f7119ea"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thủ đô của Malaysia là Kuala Lumpur.\n",
            "----------------------\n",
            "Thủ đô của Trung Quốc là Bắc Kinh (Beijing).\n"
          ]
        }
      ]
    }
  ]
}