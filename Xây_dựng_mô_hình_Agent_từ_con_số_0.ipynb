{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "name": "Xây dựng mô hình Agent từ con số 0.ipynb"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4111226d"
      },
      "outputs": [],
      "source": [
        "#%pip install openai numpy matplotlib pydantic --upgrade --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31edb044-9542-431b-b1e2-4c8d7bb5ad74"
      },
      "source": [
        "# Simple Agentic Loop Example with the Responses API (Vòng lặp khi Agent gọi phản hồi từ API)\n",
        "\n",
        "This notebook demonstrates how to build a simple agent using the OpenAI Responses API with an agentic loop.\n",
        "\n",
        "We create **two agents**:\n",
        "\n",
        "1. **Simple Agent:** (Agent gửi request và check xem request có phải function hay ko, nếu có gọi function và thêm vào phản hồi; vòng lặp sẽ dừng lại khi response ko còn tool call và bao gồm text cuối cùng)\n",
        "   - The agent enters a loop where it sends a request and checks whether the response contains a tool call.\n",
        "   - If so, it executes the function (e.g. _get_weather_) and appends the result to the conversation.\n",
        "   - The loop stops when there are no more tool calls and the response contains final text (`output_text`).\n",
        "\n",
        "2. **Objective-Based Agent:** (dừng khi đạt mục tiêu riêng (ví dụ: đã thu thập đủ 5 thành phố, hoặc có chữ “task complete”))\n",
        "   - The agent is given a custom objective function (e.g. check whether the phrase \"task complete\" is in the output).\n",
        "   - It loops until the objective function returns `True`.\n",
        "\n",
        "Below, you'll see the code for each agent along with explanations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5d4cd2e-a072-4d0b-8d1f-ddf6e5c4e2a1"
      },
      "source": [
        "## Prerequisites\n",
        "\n",
        "Make sure you have the OpenAI Python package installed and have set up authentication with your API key. Also, you should have any supporting code (for example, a real implementation of `get_weather`) ready.\n",
        "\n",
        "For demonstration, we'll define a simple `get_weather` function which calls a public weather API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3625b112"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "import json\n",
        "import requests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c8755f2e"
      },
      "outputs": [],
      "source": [
        "MODEL = \"gpt-4o-mini\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7645c517-f11e-4a8b-b375-6a2bdc90b213"
      },
      "outputs": [],
      "source": [
        "def get_weather(latitude, longitude):\n",
        "    # Hàm lấy dữ liệu thời tiết từ trang web public\n",
        "    response = requests.get(\n",
        "        f\"https://api.open-meteo.com/v1/forecast?latitude={latitude}&longitude={longitude}&current=temperature_2m\"\n",
        "    )\n",
        "    data = response.json()\n",
        "    return data['current']['temperature_2m']\n",
        "\n",
        "client = OpenAI(\n",
        "    api_key=\"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\"\n",
        ")\n",
        "\n",
        "# Tool bắt buộc phải tự viết dưới dạng file json để chạy function lấy dữ liệu thời tiết\n",
        "weather_tool = {\n",
        "    \"type\": \"function\", # loại: function\n",
        "    \"name\": \"get_weather\", # tên hàm: get_weather\n",
        "    \"description\": \"Get current temperature for provided coordinates in celsius.\", # mô tả chức năng của hàm\n",
        "    \"parameters\": { # format của các thông số\n",
        "        \"type\": \"object\", # toàn bộ input phải là 1 object (kiểu dictionary trong Python)\n",
        "        \"properties\": { # các trường của object\n",
        "            \"latitude\": { \"type\": \"number\", \"description\": \"Latitude of the location.\" }, # mỗi tham số cần có kiểu dữu liệu và mô tả chức năng\n",
        "            \"longitude\": { \"type\": \"number\", \"description\": \"Longitude of the location.\" }\n",
        "        },\n",
        "        \"required\": [\"latitude\", \"longitude\"], # bắt buộc phải có 2 trường lat và long\n",
        "        \"additionalProperties\": False # ko cho phép thêm bất cứ trường nào\n",
        "    },\n",
        "    \"strict\": True\n",
        "}\n",
        "tools = [weather_tool]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34d16527-c9fc-41b1-90f8-9f1427bd254e"
      },
      "source": [
        "## Agent 1: Simple Agent Loop\n",
        "\n",
        "This agent sends a prompt (asking about the weather), then enters an agentic loop.\n",
        "\n",
        "At each turn, it calls the Responses API:\n",
        "\n",
        "- **If the response contains a tool call:** The agent executes the function (using our `get_weather` tool) and appends the function result to the conversation as a new message.\n",
        "- **If the response provides output text:** The agent stops, printing the final output.\n",
        "\n",
        "Below is the code for the simple agent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7dafb74f-5a71-4f7e-a0a1-859a4d6c1a2c",
        "outputId": "47666c84-3473-4746-ae13-543c01437faf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Executed get_weather: Result = 18.3°C\n",
            "Final Agent Output: The temperature in Paris today is 18.3°C.\n"
          ]
        }
      ],
      "source": [
        "messages = [{\"role\": \"developer\", \"content\": \"What's the weather like in Paris today?\"}]\n",
        "\n",
        "while True: # Cứ chạy tiếp tục vòng lặp\n",
        "    response = client.responses.create(\n",
        "        model=MODEL,\n",
        "        input=messages,\n",
        "        tools=tools # tools[1]: theo cấu trúc; tools[2]: tools như bên trên khai báo, chính là [weather_tool]\n",
        "    )\n",
        "    # Các trường hợp output của response:\n",
        "    if response.output:\n",
        "        for output_item in response.output: # Với mỗi output trong phản hồi:\n",
        "            if hasattr(output_item, 'type') and output_item.type == \"function_call\": # Nếu output trong phản hồi là dạng \"function_call\" (hàm hasattr(a,b) gồm 2 thành phần)\n",
        "                # Thêm lời gọi hàm function call vào messages:\n",
        "                messages.append(output_item)\n",
        "\n",
        "                tool_call = output_item\n",
        "                args = json.loads(tool_call.arguments) # Load các tham số\n",
        "\n",
        "                # Execute the function, e.g. get_weather (simulate using our get_weather function): Code backend bắt được yêu cầu gọi hàm (function_call)\n",
        "                result = get_weather(args['latitude'], args['longitude'])\n",
        "                print(f\"Executed {tool_call.name}: Result = {result}°C\") # Kết quả của hàm trong back-end\n",
        "\n",
        "                # Mặc dù ra được output nhưng LLM chưa biết kết quả. Nếu ko trả ngược kết quả này vào cuộc hội thoại, LLM sẽ bị \"mù thông tin\" và không thể dùng được.\n",
        "                messages.append({ # Trả ngược kết quả về cho LLM, truyền vào đoạn hội thoại\n",
        "                    \"type\": \"function_call_output\",\n",
        "                    \"call_id\": tool_call.call_id, # Gắn ID để LLM biết output này thuộc về function call nào\n",
        "                    \"output\": str(result)\n",
        "                })\n",
        "\n",
        "    # Nếu output của response là dạng text, kết thúc vòng lặp và cho ra kết quả\n",
        "    if hasattr(response, 'output_text') and response.output_text:\n",
        "        print(\"Final Agent Output:\", response.output_text)\n",
        "        break\n",
        "\n",
        "    # For simplicity, break if no further tool calls are made\n",
        "    if not response.output:\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e7655211"
      },
      "outputs": [],
      "source": [
        "# Viết thành hàm agent_loop:\n",
        "def agent_loop(messages, tools):\n",
        "    while True:\n",
        "        response = client.responses.create(\n",
        "            model=MODEL,\n",
        "            input=messages,\n",
        "            tools=tools\n",
        "        )\n",
        "\n",
        "        # Process all function calls in the response\n",
        "        if response.output:\n",
        "            for output_item in response.output:\n",
        "                if hasattr(output_item, 'type') and output_item.type == \"function_call\":\n",
        "                    # Append the function call to the messages:\n",
        "                    messages.append(output_item)\n",
        "\n",
        "                    tool_call = output_item\n",
        "                    args = json.loads(tool_call.arguments)\n",
        "\n",
        "                    # Execute the function, e.g. get_weather (simulate using our get_weather function)\n",
        "                    result = get_weather(args['latitude'], args['longitude'])\n",
        "                    print(f\"Executed {tool_call.name}: Result = {result}°C\")\n",
        "\n",
        "                    # Append the function call output to the conversation\n",
        "                    messages.append({\n",
        "                        \"type\": \"function_call_output\",\n",
        "                        \"call_id\": tool_call.call_id,\n",
        "                        \"output\": str(result)\n",
        "                    })\n",
        "\n",
        "        # If the final output text is provided, break the loop\n",
        "        if hasattr(response, 'output_text') and response.output_text:\n",
        "            print(\"Final Agent Output:\", response.output_text)\n",
        "            break\n",
        "\n",
        "        # Otherwise, continue the loop (in a full implementation, you might update the conversation further)\n",
        "\n",
        "        # For simplicity, break if no further tool calls are made\n",
        "        if not response.output:\n",
        "            break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0bd289eb"
      },
      "outputs": [],
      "source": [
        "# Prompt trong trường hợp khác\n",
        "messages = [{\"role\": \"developer\", \"content\": \"What's the weather like in Paris today? Before replying I want you to also get the weather for Berlin.\"}]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "49e422ff",
        "outputId": "273f5834-01f8-40b6-d5c8-cf2439b2be2b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Agent Output: Today in Paris, the temperature is approximately **18.3°C**. Meanwhile, in Berlin, it’s about **15.3°C**.\n"
          ]
        }
      ],
      "source": [
        "agent_loop(messages=messages, tools=tools)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ed857dce-9b25-4490-8516-1a043d6b2ff8"
      },
      "source": [
        "---\n",
        "\n",
        "## Agent 2: Agent with Custom Objective Function\n",
        "\n",
        "This agent uses a custom objective function to decide whether to continue looping. In this example, the objective function checks if the agent's output text contains the phrase \"task complete\".\n",
        "\n",
        "The agent will continue to request responses (and execute any tool calls) until the objective is met. Note that this is a simplified demonstration intended for teaching purposes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0f937e76-c5e3-4b04-b3c3-395fd103ae3b",
        "outputId": "4bb44afd-817d-4b58-8bff-12f91856457c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 517
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Executed get_weather: 19.2°C\n",
            "Executed get_weather: 19.6°C\n",
            "Executed get_weather: 17.0°C\n",
            "Executed get_weather: 17.2°C\n",
            "Executed get_weather: 32.2°C\n",
            "Agent says: Task complete.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RateLimitError",
          "evalue": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-oTVPuEmELabHRM00lhbg59iB on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-892300733.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     response = client.responses.create(\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMODEL\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/resources/responses/responses.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, background, include, input, instructions, max_output_tokens, max_tool_calls, metadata, model, parallel_tool_calls, previous_response_id, prompt, prompt_cache_key, reasoning, safety_identifier, service_tier, store, stream, stream_options, temperature, text, tool_choice, tools, top_logprobs, top_p, truncation, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    771\u001b[0m         \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mhttpx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTimeout\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mNotGiven\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNOT_GIVEN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    772\u001b[0m     ) -> Response | Stream[ResponseStreamEvent]:\n\u001b[0;32m--> 773\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m    774\u001b[0m             \u001b[0;34m\"/responses\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    775\u001b[0m             body=maybe_transform(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1257\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1258\u001b[0m         )\n\u001b[0;32m-> 1259\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1261\u001b[0m     def patch(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1045\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m                 \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Re-raising status error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1047\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1048\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1049\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-oTVPuEmELabHRM00lhbg59iB on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "# Hàm chạy tối đa với max_search = 5:\n",
        "def objective_met(search_count, max_searches=5):\n",
        "    # Dừng lại khi search_count <= max_search\n",
        "    return search_count > max_searches\n",
        "\n",
        "# Initial conversation: developer sets the task, user gives the first city\n",
        "messages= [\n",
        "    {\n",
        "        \"role\": \"developer\",\n",
        "        \"content\": (\n",
        "            \"Your goal is to gather weather for at least 5 different cities. \"\n",
        "            \"Once you've done that, respond with 'task complete'.\"\n",
        "        )\n",
        "    },\n",
        "    {\"role\": \"user\", \"content\": \"Search the weather in Berlin.\"}\n",
        "]\n",
        "\n",
        "search_count = 0\n",
        "\n",
        "while True:\n",
        "    response = client.responses.create(\n",
        "        model=MODEL,\n",
        "        input=messages,\n",
        "        tools=tools\n",
        "    )\n",
        "\n",
        "    # 1) Handle any function calls (e.g. get_weather)\n",
        "    for item in response.output or []:\n",
        "        if getattr(item, 'type', None) == \"function_call\":\n",
        "            messages.append(item)  # pass the function call back into context\n",
        "\n",
        "            # parse and execute\n",
        "            args = json.loads(item.arguments)\n",
        "            temp = get_weather(args['latitude'], args['longitude'])\n",
        "            print(f\"Executed {item.name}: {temp}°C\")\n",
        "\n",
        "            # append function result\n",
        "            messages.append({\n",
        "                \"type\": \"function_call_output\",\n",
        "                \"call_id\": item.call_id,\n",
        "                \"output\": str(temp)\n",
        "            })\n",
        "\n",
        "            search_count += 1\n",
        "\n",
        "    # 2) Check for assistant output_text\n",
        "    if hasattr(response, 'output_text') and response.output_text:\n",
        "        text = response.output_text\n",
        "        print(\"Agent says:\", text)\n",
        "        messages.append({\"role\": \"assistant\", \"content\": text})\n",
        "\n",
        "    # 3) Check objective\n",
        "    if objective_met(search_count):\n",
        "        print(f\"Objective met: searched {search_count} cities. Task complete.\")\n",
        "        break\n",
        "\n",
        "    # 4) If not done, prompt for the next city\n",
        "    messages.append({\n",
        "        \"role\": \"user\",\n",
        "        \"content\": f\"We've searched {search_count} so far. Please search another city.\"\n",
        "    })\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8f57fd6f"
      },
      "outputs": [],
      "source": [
        "print(messages[-1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9e0e6c0-5e20-488f-8d8e-0d7718af3bf0"
      },
      "source": [
        "## Summary\n",
        "\n",
        "- **Agent 1 (Simple Agent):**\n",
        "  - Uses a while loop to repeatedly call the Responses API.\n",
        "  - Checks for tool (function) calls, executes them, and appends the output to the conversation.\n",
        "  - The loop stops when a final response (output_text) is provided and there are no more tool calls.\n",
        "\n",
        "- **Agent 2 (Objective-Based Agent):**\n",
        "  - Uses a custom objective function (`objective_met`) to decide when to stop the loop.\n",
        "  - Continues to gather responses and execute tools until the agent's output includes a key phrase (\"task complete\").\n",
        "\n",
        "Both agents illustrate how you can build agentic loops from scratch using the Responses API and custom tools. Adapt and extend these examples as needed for more complex tasks!"
      ]
    }
  ]
}