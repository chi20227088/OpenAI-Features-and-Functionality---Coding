{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "72db3b23",
        "outputId": "91d31a30-99f6-48e5-983b-8b88f6168e5d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.2/91.2 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m787.8/787.8 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m44.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m46.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.3.1 which is incompatible.\n",
            "dask-cudf-cu12 25.6.0 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.1 which is incompatible.\n",
            "cudf-cu12 25.6.0 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "%pip install openai tiktoken pandas scikit-learn matplotlib --upgrade --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99136eba"
      },
      "source": [
        "# Introduction to Vector Embeddings\n",
        "\n",
        "In this notebook, we’ll cover the theory behind text embeddings, show you how to generate and use them with the OpenAI API, and then give you a hands-on exercise to try embedding search yourself."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d004dfbe"
      },
      "source": [
        "## Theory: What Are Embeddings?\n",
        "- An embedding is a high‑dimensional vector that captures the semantic meaning of text.\n",
        "- Similar pieces of text have vectors that are close together in vector space (cosine similarity, dot product).\n",
        "- Common use cases: search, clustering, recommendation, anomaly detection, classification."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ae96f9b"
      },
      "source": [
        "## Getting Started: Install & Import\n",
        "Make sure you have the OpenAI and tiktoken packages installed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "d7dd0d13"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from openai import OpenAI\n",
        "from sklearn.decomposition import PCA\n",
        "import tiktoken"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8bbd90f"
      },
      "source": [
        "## 1. Generate an Embedding\n",
        "Use `text-embedding-3-small` to turn text into a 1536‑dimensional vector."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "eb7e6d5c"
      },
      "outputs": [],
      "source": [
        "# Initialize the client\n",
        "# Note: In a real application, you would use an environment variable or secure method\n",
        "# to store your API key. This is just for demonstration.\n",
        "client = OpenAI(\n",
        "    # Replace with your actual API key or use: api_key=os.environ.get(\"OPENAI_API_KEY\")\n",
        "    api_key=\"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\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "d3415513",
        "outputId": "3eff57e8-1a0f-4691-fca8-4d5d59961372",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vector length: 1536\n",
            "First 5 dims: [-0.0183968  -0.0072255   0.00362544 -0.05420079 -0.02269785]\n"
          ]
        }
      ],
      "source": [
        "# Hàm chuyển text thành vecto, sử dụng model='text-embedding-3-small'\n",
        "def get_embedding(text, model='text-embedding-3-small'):\n",
        "    text_clean = text.replace('\\n', ' ') # Loại bỏ các ký tự k cần thiết: xuống dòng, khoảng trắng\n",
        "    resp = client.embeddings.create(input=[text_clean], model=model) # tạo vecto\n",
        "    return np.array(resp.data[0].embedding) # Lưu thành 1 mảng\n",
        "\n",
        "sample = 'The quick brown fox jumps over the lazy dog.'\n",
        "emb = get_embedding(sample)\n",
        "print(f'Vector length: {len(emb)}')\n",
        "print('First 5 dims:', emb[:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87ba61f6"
      },
      "source": [
        "## 2. Count Tokens Before Embedding\n",
        "Use `tiktoken` to estimate input size and control cost."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "5aa498fd",
        "outputId": "a5259d34-2545-444f-b5c7-3eeff27c2f0e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample token count: 10\n"
          ]
        }
      ],
      "source": [
        "# Hàm đếm số token có trong câu\n",
        "def num_tokens_from_string(s: str, encoding_name: str = 'cl100k_base') -> int:\n",
        "    enc = tiktoken.get_encoding(encoding_name) # encoding thành các tokens\n",
        "    return len(enc.encode(s))\n",
        "\n",
        "print('Sample token count:', num_tokens_from_string(sample))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "520a2371"
      },
      "source": [
        "## 3. Dimensionality Reduction & Visualization\n",
        "Project a few sentence embeddings down to 2D with PCA.\n",
        "\n",
        "# Ý tưởng\n",
        "\n",
        "- Bạn có nhiều câu (sentences).\n",
        "\n",
        "- Mỗi câu được chuyển thành một vector embedding (bằng hàm get_embedding(s)).\n",
        "\n",
        "- Các vector này thường có số chiều lớn (ví dụ 768, 1024...).\n",
        "\n",
        "- Dùng PCA để giảm toàn bộ xuống 2 chiều (2D).\n",
        "\n",
        "- Trực quan hóa các câu trên mặt phẳng 2D — giúp “nhìn thấy” sự gần nhau/khác biệt giữa ý nghĩa các câu."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "847c497f",
        "outputId": "36371deb-c361-4b3e-f587-675053c871ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nplt.figure(figsize=(8,6))\\nplt.scatter(points[:,0], points[:,1])\\nfor i, txt in enumerate(sentences):\\n    plt.annotate(txt, (points[i,0], points[i,1]))\\nplt.title('2D PCA of Sentence Embeddings')\\nplt.xlabel('PC1')\\nplt.ylabel('PC2')\\nplt.show()\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "sentences = [\n",
        "    'I love machine learning',\n",
        "    'OpenAI creates powerful AI models',\n",
        "    'The sky is clear today',\n",
        "    'I enjoy hiking in the mountains',\n",
        "    'This restaurant has great food'\n",
        "]\n",
        "vectors = np.vstack([get_embedding(s) for s in sentences]) # Chuyển các câu trên thành vecto Embedding\n",
        "\n",
        "pca = PCA(n_components=2) # Giảm số chiều của vecto = 2\n",
        "points = pca.fit_transform(vectors)\n",
        "\n",
        "'''\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.scatter(points[:,0], points[:,1])\n",
        "for i, txt in enumerate(sentences):\n",
        "    plt.annotate(txt, (points[i,0], points[i,1]))\n",
        "plt.title('2D PCA of Sentence Embeddings')\n",
        "plt.xlabel('PC1')\n",
        "plt.ylabel('PC2')\n",
        "plt.show()\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2a897143"
      },
      "source": [
        "---\n",
        "\n",
        "## Exercise for You\n",
        "1. Pick 5 of your own short sentences.\n",
        "2. Embed them using the `get_embedding` function.\n",
        "3. Compute pairwise cosine similarities and identify the two most similar sentences.\n",
        "4. (Bonus) Visualize them in 2D with PCA as above."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "sentence = ['Tôi thích đọc sách', 'Tôi thích nghe nhạc', 'Anh ấy đang chạy bộ', 'Mẹ tôi làm nghề giáo viên']\n",
        "vectors = np.vstack([get_embedding(s) for s in sentence])\n",
        "print(vectors)\n",
        "\n",
        "similarity = cosine_similarity(vectors)\n",
        "print(similarity)\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Đặt giá trị trên đường chéo thành -1 để loại bỏ so sánh chính nó\n",
        "np.fill_diagonal(similarity, -1)\n",
        "# Tìm vị trí của giá trị lớn nhất\n",
        "idx = np.unravel_index(np.argmax(similarity), similarity.shape) #argmax lấy giá trị max; unravel viết lại index, ví dụ index = 13 biến đổi về tọa độ gồm 2 biến\n",
        "print(f\"\\nHai câu giống nhau nhất là:\\n- \\\"{sentence[idx[0]]}\\\"\\n- \\\"{sentence[idx[1]]}\\\"\")\n",
        "print(f\"Cosine similarity: {similarity[idx]:.4f}\")\n"
      ],
      "metadata": {
        "id": "ZuokhNzmkJQS",
        "outputId": "89230e9d-9018-437b-e476-e93922edbfdb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0.01620053  0.0004295  -0.08704468 ...  0.00370571  0.00337373\n",
            "  -0.00143892]\n",
            " [ 0.01901255 -0.02254858 -0.09381574 ...  0.00332463  0.00497948\n",
            "  -0.01252983]\n",
            " [ 0.02310837  0.0010586  -0.03352727 ... -0.00077936 -0.0163608\n",
            "  -0.01691928]\n",
            " [ 0.00689444 -0.00827906 -0.05735124 ... -0.01935195 -0.01881121\n",
            "  -0.05830163]]\n",
            "[[1.         0.60576539 0.37345729 0.36652316]\n",
            " [0.60576539 1.         0.34801537 0.32130442]\n",
            " [0.37345729 0.34801537 1.         0.3559856 ]\n",
            " [0.36652316 0.32130442 0.3559856  1.        ]]\n",
            "\n",
            "Hai câu giống nhau nhất là:\n",
            "- \"Tôi thích đọc sách\"\n",
            "- \"Tôi thích nghe nhạc\"\n",
            "Cosine similarity: 0.6058\n"
          ]
        }
      ]
    }
  ]
}